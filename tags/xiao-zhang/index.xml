<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiao Zhang on Security Research Group</title>
    <link>//uvasrg.github.io/tags/xiao-zhang/</link>
    <description>Recent content in Xiao Zhang on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Mon, 16 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="//uvasrg.github.io/tags/xiao-zhang/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NeurIPS 2019</title>
      <link>//uvasrg.github.io/neurips2019/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/neurips2019/</guid>
      <description> Here&#39;s a video of Xiao Zhang&#39;s presentation at NeurIPS 2019: https://slideslive.com/38921718/track-2-session-1 (starting at 26:50)  See this post for info on the paper. Here are a few pictures from NeurIPS 2019 (by Sicheng Zhu and Mohammad Mahmoody):   




 </description>
    </item>
    
    <item>
      <title>NeurIPS 2019: Empirically Measuring Concentration</title>
      <link>//uvasrg.github.io/neurips-2019-empirically-measuring-concentration/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/neurips-2019-empirically-measuring-concentration/</guid>
      <description>Xiao Zhang will present our work (with Saeed Mahloujifar and Mohamood Mahmoody) as a spotlight at NeurIPS 2019, Vancouver, 10 December 2019.
Recent theoretical results, starting with Gilmer et al.&amp;lsquo;s Adversarial Spheres (2018), show that if inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable.c The key insight from this line of research is that concentration of measure gives lower bound on adversarial risk for a large collection of classifiers (e.</description>
    </item>
    
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>Five students from our group presented posters at the department&amp;rsquo;s Fall Research Symposium:
 
Anshuman Suri&#39;s Overview Talk   Bargav Jayaraman, Evaluating Differentially Private Machine Learning In Practice [Poster]
[Paper (USENIX Security 2019)]  

 Hannah Chen [Poster]  

 Xiao Zhang [Poster]
[Paper (NeurIPS 2019)]  

 Mainudding Jonas [Poster]  

 Fnu Suya [Poster]
[Paper (USENIX Security 2020)]  </description>
    </item>
    
    <item>
      <title>Cost-Sensitive Adversarial Robustness at ICLR 2019</title>
      <link>//uvasrg.github.io/cost-sensitive-adversarial-robustness-at-iclr-2019/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/cost-sensitive-adversarial-robustness-at-iclr-2019/</guid>
      <description>Xiao Zhang will present Cost-Sensitive Robustness against Adversarial Examples on May 7 (4:30-6:30pm) at ICLR 2019 in New Orleans.
   Paper: [PDF] [OpenReview] [ArXiv]</description>
    </item>
    
    <item>
      <title>Empirically Measuring Concentration</title>
      <link>//uvasrg.github.io/empirically-measuring-concentration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/empirically-measuring-concentration/</guid>
      <description>Xiao Zhang and Saeed Mahloujifar will present our work on Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness at two workshops May 6 at ICLR 2019 in New Orleans: Debugging Machine Learning Models and Safe Machine Learning: Specification, Robustness and Assurance.
Paper: [PDF]
   </description>
    </item>
    
    <item>
      <title>ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</title>
      <link>//uvasrg.github.io/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</guid>
      <description>Xiao Zhang and my paper on Cost-Sensitive Robustness against Adversarial Examples has been accepted to ICLR 2019.
Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. However, these methods assume that all the adversarial transformations provide equal value for adversaries, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier&amp;rsquo;s performance for specific tasks.</description>
    </item>
    
  </channel>
</rss>